
<!doctype html>

<html>
<head>
  <meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, user-scalable=yes">
  <meta name="theme-color" content="#4F7DC9">
  <meta charset="UTF-8">
  <title>Airflow installation tutorial</title>
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Code+Pro:400|Roboto:400,300,400italic,500,700|Roboto+Mono">
  <link rel="stylesheet" href="//fonts.googleapis.com/icon?family=Material+Icons">
  <link rel="stylesheet" href="https://storage.googleapis.com/codelab-elements/codelab-elements.css">
  <style>
    .success {
      color: #1e8e3e;
    }
    .error {
      color: red;
    }
  </style>
</head>
<body>
  <google-codelab-analytics gaid="UA-49880327-14"></google-codelab-analytics>
  <google-codelab codelab-gaid=""
                  id="airflow-setup"
                  title="Airflow installation tutorial"
                  environment="web"
                  feedback-link="https://github.com/kirenz/codelabs/blob/master/markdown/airflow-setup">
    
      <google-codelab-step label="Overview" duration="1">
        <h2 is-upgraded>What we cover</h2>
<p>In this tutorial we are going to install <a href="https://airflow.apache.org/docs/apache-airflow/stable/start/local.html" target="_blank">Apache Airflow</a> on your system. Furthermore, we will implement a basic pipeline.</p>
<p class="image-container"><img alt="Apache Airflow logo" style="width: 200.00px" src="img/86832b628c8aabfe.png"></p>
<ul>
<li>Airflow is a open source platform to programmatically author, schedule and monitor workflows.</li>
<li>Airflow pipelines are defined in Python, allowing for dynamic pipeline generation. This allows for writing code that instantiates pipelines dynamically.</li>
<li>Anyone with Python knowledge can deploy a workflow with Airflow. Apache Airflow does not limit the scope of your pipelines; you can use it to build ML models, transfer data, manage your infrastructure, and more.</li>
</ul>
<aside class="special"><p> Monitor, schedule and manage your workflows via a robust and modern web application.  </p>
</aside>


      </google-codelab-step>
    
      <google-codelab-step label="Prerequisites for Windows" duration="5">
        <h2 is-upgraded>Windows Subsystem for Linux 2 (WSL2)</h2>
<p>If you have a Windows machine, you need Windows Subsystem for Linux 2 (WSL2). Here, we follow the instructions provided by Microsoft&#39;s Craig Loewen to set up WSL2 (see <a href="https://devblogs.microsoft.com/commandline/install-wsl-with-a-single-command-now-available-in-windows-10-version-2004-and-higher/" target="_blank">this post</a> to learn more).</p>
<ul>
<li>Open a command prompt window with admin privileges and run</li>
</ul>
<pre><code language="language-Bash" class="language-Bash">wsl.exe --install
</code></pre>
<p>This will automatically install the open source operating system Ubuntu and the latest WSL Linux kernel version onto your machine.</p>
<ul>
<li>When it&#39;s completed, restart your machine.</li>
</ul>
<p>Your distribution will start after you boot up again, completing the installation.</p>
<ul>
<li>You can launch the Linux terminal with one of the following options: <ul>
<li>(1) Install the Windows Terminal from the <a href="https://apps.microsoft.com/store/detail/windows-terminal/9N0DX20HK701?hl=de-de&gl=DE" target="_blank">Microsoft Store</a> (recommend option)</li>
<li>(2) Use the Ubuntu icon or</li>
<li>(3) enter <code>wsl</code> or <code>bash</code> in Powershell.</li>
</ul>
</li>
</ul>
<p><em>You can use </em></p>
<p><em><code>wsl --update</code></em></p>
<p><em> to manually update your WSL Linux kernel, and you can use </em></p>
<p><em><code>wsl --update rollback</code></em></p>
<p><em> to rollback to a previous WSL Linux kernel version. To learn more about WSL, take a look at this post form Microsoft: </em><a href="https://docs.microsoft.com/en-us/windows/wsl/about#what-is-wsl-2" target="_blank"><em>&#34;What is the Windows Subsystem for Linux?&#34;</em></a><em>.</em></p>
<h2 is-upgraded>Install Miniforge</h2>
<p>Next, we install Miniforge (an alternative to Anaconda and Miniconda) on your Linux system.</p>
<ul>
<li>Launch the Linux terminal with one of the following options: <ul>
<li>(1) Install the Windows Terminal from the <a href="https://apps.microsoft.com/store/detail/windows-terminal/9N0DX20HK701?hl=de-de&gl=DE" target="_blank">Microsoft Store</a> (recommend option)</li>
<li>(2) Use the Ubuntu icon or</li>
<li>(3) enter <code>wsl</code> or <code>bash</code> in Powershell.</li>
</ul>
</li>
</ul>
<p>Next, we install Miniforge with <code>wget</code> (we use <code>wget</code> to download directly from the terminal).</p>
<ul>
<li>Get the appropriate Linux version of Miniforge3 for your machine (see <a href="https://github.com/conda-forge/miniforge/releases" target="_blank">this overview</a>; usually <code>x86_64 (amd64)</code>). Here is the example for x86_64 (amd64):</li>
</ul>
<pre><code language="language-Bash" class="language-Bash">wget https://github.com/conda-forge/miniforge/releases/download/4.12.0-0/Miniforge3-4.12.0-0-Linux-x86_64.sh
</code></pre>
<ul>
<li>Now install Miniforge from the installer script:</li>
</ul>
<pre><code language="language-Bash" class="language-Bash">sh Miniforge3-4.12.0-0-Linux-x86_64.sh
</code></pre>


      </google-codelab-step>
    
      <google-codelab-step label="Prerequisites for Apple" duration="5">
        <h2 is-upgraded>MiniForge</h2>
<p>To start this tutorial, I recommend to use <a href="https://github.com/conda-forge/miniforge" target="_blank">Miniforge</a> (a community-led alternative to Anaconda):</p>
<ul>
<li><a href="https://kirenz.github.io/codelabs/codelabs/miniforge-setup/#0" target="_blank">Miniforge3 installation tutorial</a>.</li>
</ul>


      </google-codelab-step>
    
      <google-codelab-step label="Create Virtual Environment" duration="2">
        <p>On Windows open your Linux terminal. On macOS or Linux open a terminal window.</p>
<p>We create an environment with a specific version of Python and install pip. We call the environment <code>airflow</code> (if you don&#39;t have Python 3.10 you can replace it with 3.9 or 3.8):</p>
<pre><code language="language-bash" class="language-bash">conda create -n airflow python=3.10 pip
</code></pre>
<p>When conda asks you to proceed <code>(proceed ([y]/n)?</code>), type <code>y</code>.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Installation" duration="10">
        <p>To install Airflow, we mainly follow the <a href="https://airflow.apache.org/docs/apache-airflow/stable/start/local.html" target="_blank"> installation tutorial</a> provided by Apache Airflow. Note that we use <a href="https://pip.pypa.io/en/stable/" target="_blank"><strong>pip</strong></a> to install Airflow an some additional modules in our environment. When pip asks you to proceed <code>(proceed ([y]/n)?</code>), simply type <code>y</code>.</p>
<ul>
<li>First, you need to activate your environment as follows:</li>
</ul>
<pre><code language="language-bash" class="language-bash">conda activate airflow
</code></pre>
<ul>
<li>Then upgrade pip:</li>
</ul>
<pre><code language="language-bash" class="language-bash">pip install --upgrade pip
</code></pre>
<ul>
<li>Airflow needs <code>virualenv</code> so we install it:</li>
</ul>
<pre><code language="language-bash" class="language-bash">pip install virtualenv
</code></pre>
<ul>
<li>Next, Airflow needs a home. <code>your-home-directory/airflow</code> is the default:</li>
</ul>
<p><em>Here is the command for Mac and Linux:</em></p>
<pre><code language="language-bash" class="language-bash">export AIRFLOW_HOME=~/airflow
</code></pre>
<ul>
<li>Install Airflow with the following constraints file. We use Airflow Version &#34;2.3.1&#34; and Python &#34;3.10.&#34;(if you don&#39;t have Python 3.10 you can replace it with 3.9 or 3.8):</li>
</ul>
<pre><code language="language-bash" class="language-bash">pip install &#34;apache-airflow==2.3.1&#34; --constraint &#34;https://raw.githubusercontent.com/apache/airflow/constraints-2.3.1/constraints-3.10.txt&#34;
</code></pre>
<ul>
<li>Since we will be using pandas and scikit-learn in some of our examples, we install the modules with pip:</li>
</ul>
<pre><code language="language-bash" class="language-bash">pip install pandas
</code></pre>
<pre><code language="language-bash" class="language-bash">pip install -U scikit-learn
</code></pre>
<ul>
<li>The following <code>airflow standalone</code> command will <ul>
<li>(1) initialise a SQLite database,</li>
<li>(2) make a user, and</li>
<li>(3) start all components</li>
</ul>
</li>
</ul>
<pre><code language="language-bash" class="language-bash">airflow standalone
</code></pre>
<p>We only run this command once when we install Airflow. If you want to run the individual parts of Airflow manually rather than using the all-in-one standalone command, check out the instructions provided <a href="https://airflow.apache.org/docs/apache-airflow/stable/start/local.html" target="_blank">here</a>.</p>
<aside class="warning"><p> If you get the error message &#34;AttributeError: &#39;NoneType object has no attribute is_alive&#34; stop the process with Ctrl + c and use the command airflow standalone one more time. </p>
</aside>
<ul>
<li>In the terminal output: Look for the provided <code>username</code> and <code>password</code> and store them somewhere</li>
<li>Open the Airflow UI in your browser (ideally in Chrome) <a href="http://0.0.0.0:8080" target="_blank">http://0.0.0.0:8080</a> and provide <code>username</code> and <code>password</code>.</li>
<li>The Airflow UI makes it easy to monitor and troubleshoot your data pipelines. Here&#39;s a quick overview of some of the features and visualizations you can find: <a href="https://airflow.apache.org/docs/apache-airflow/stable/ui.html#" target="_blank">Airflow UI</a></li>
<li>If you are done:<ol type="1">
<li>Log out from the user menu,</li>
<li>Go to your terminal and stop the Airflow process with <code>Ctrl</code>+<code>c</code> (this will shut down components).</li>
</ol>
<br>Later, we will restart Airflow by using different commands.</li>
</ul>


      </google-codelab-step>
    
      <google-codelab-step label="Start Airflow" duration="2">
        <p>In this section, we take a look at how to start Airflow:</p>
<ul>
<li>Start your terminal and activate your <code>airflow</code> environment if needed</li>
</ul>
<pre><code language="language-Bash" class="language-Bash">conda activate airflow
</code></pre>
<ul>
<li>Export the airflow home variable</li>
</ul>
<pre><code language="language-Bash" class="language-Bash">export AIRFLOW_HOME=~/airflow
</code></pre>
<ul>
<li>Start the Airflow webserver</li>
</ul>
<pre><code language="language-Bash" class="language-Bash">airflow webserver
</code></pre>
<ul>
<li>Open the Airflow UI in your browser (ideally in Chrome) <a href="http://0.0.0.0:8080" target="_blank">http://0.0.0.0:8080</a> and provide your <code>username</code> and <code>password</code>.</li>
<li>If you are done:<ol type="1">
<li>Log out from the user menu,</li>
<li>Go to your terminal and stop the Airflow process with <code>Ctrl</code>+<code>c</code> (this will shut down all components).</li>
</ol>
</li>
</ul>


      </google-codelab-step>
    
      <google-codelab-step label="First pipeline" duration="5">
        <p>Here, we mainly follow the instructions provided in this <a href="https://airflow.apache.org/docs/apache-airflow/stable/tutorial.html#tutorial" target="_blank">Apache Airflow tutorial</a>:</p>
<ul>
<li>First, create a new folder called <code>dags</code> in you airflow home (i.e. <code>~/airflow/dags</code>).</li>
<li>Copy <a href="https://github.com/kirenz/airflow/blob/main/my_airflow_dag.py" target="_blank">this Python script</a> and save it as <code>my_airflow_dag.py</code> in your <code>~/airflow/dags</code> folder.</li>
</ul>
<aside class="warning"><p> The file my_airflow_dag needs to be stored in the DAGs folder referenced in your airflow.cfg. The default location for your DAGs is ~/airflow/dags. </p>
</aside>
<ul>
<li>Open a new terminal window and activate your <code>airflow</code> environment if needed</li>
</ul>
<pre><code language="language-Bash" class="language-Bash">conda activate airflow
</code></pre>
<ul>
<li>Export the airflow home variable</li>
</ul>
<pre><code language="language-Bash" class="language-Bash">export AIRFLOW_HOME=~/airflow
</code></pre>
<ul>
<li>Now run the following command:</li>
</ul>
<pre><code language="language-bash" class="language-bash">python ~/airflow/dags/my_airflow_dag.py
</code></pre>
<p>If the script does not raise an exception it means that you have not done anything wrong, and that your Airflow environment is somewhat sound.</p>
<ul>
<li>Now proceed to the next step.</li>
</ul>
<p><em>If you want to learn more about the content of the my_airflow-dag.py script, review </em><a href="https://airflow.apache.org/docs/apache-airflow/stable/tutorial.html" target="_blank"><em>the Airflow tutorial</em></a><em>.</em></p>


      </google-codelab-step>
    
      <google-codelab-step label="Command Line Metadata Validation" duration="4">
        <p>First, we use the command line to do some metadata validation. Let&#39;s run a few commands in your terminal to test your script:</p>
<ul>
<li>Initialize the database tables</li>
</ul>
<pre><code language="language-bash" class="language-bash">airflow db init
</code></pre>
<ul>
<li>Print the list of active DAGs (there are many example DAGs provided by Airflow)</li>
</ul>
<pre><code language="language-bash" class="language-bash">airflow dags list
</code></pre>
<ul>
<li>Print the list of tasks in the &#34;my_airflow_dag&#34;</li>
</ul>
<pre><code language="language-bash" class="language-bash">airflow tasks list my_airflow_dag
</code></pre>
<ul>
<li>Print the hierarchy of tasks in the &#34;my_airflow_dag&#34; DAG</li>
</ul>
<pre><code language="language-bash" class="language-bash">airflow tasks list my_airflow_dag --tree
</code></pre>


      </google-codelab-step>
    
      <google-codelab-step label="Testing single tasks" duration="4">
        <p>Let&#39;s start our tests by running one actual task instance for a specific date (independent of other tasks).</p>
<p>The date specified in this context is called the &#34;logical date&#34; (also called execution date), which simulates the scheduler running your task or DAG for a specific date and time, even though it physically will run now (or as soon as its dependencies are met).</p>
<aside class="special"><p> The scheduler runs your task for a specific date and time, not at a specific date.  </p>
</aside>
<p>This is because each run of a DAG conceptually represents not a specific date and time, but an interval between two times, called a <em>data interval</em>. A DAG run&#39;s logical date is the start of its data interval.</p>
<p>The general command layout is as follows:</p>
<pre><code language="language-Bash" class="language-Bash">command subcommand dag_id task_id date
</code></pre>
<ul>
<li>Testing <code>task_print_date</code>:</li>
</ul>
<pre><code language="language-bash" class="language-bash">airflow tasks test my_airflow_dag task_print_date 2021-05-20
</code></pre>
<p><em>Take a look at the last lines in the output (ignore warnings for now)</em></p>
<ul>
<li>Testing <code>task_sleep</code></li>
</ul>
<pre><code language="language-bash" class="language-bash">airflow tasks test my_airflow_dag task_sleep 2021-05-20
</code></pre>
<ul>
<li>Testing <code>task_templated</code></li>
</ul>
<pre><code language="language-Bash" class="language-Bash">airflow tasks test my_airflow_dag task_templated 2021-05-20
</code></pre>
<p>Everything looks like it&#39;s running fine so let&#39;s run a backfill.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Backfill" duration="4">
        <p><code>backfill</code> will respect your dependencies, emit logs into files and talk to the database to record status.</p>
<p>If you do have a <em>webserver</em> up, you will be able to track the progress.</p>
<p><code>airflow webserver</code> will start a web server if you are interested in tracking the progress visually as your backfill progresses.</p>
<ul>
<li>The date range in this context is a <code>start_date</code> and optionally an <code>end_date</code>, which are used to populate the run schedule with task instances from this dag.</li>
<li>Start your backfill on a date range</li>
</ul>
<pre><code language="language-Bash" class="language-Bash">airflow dags backfill my_airflow_dag \
    --start-date 2021-05-20 \
    --end-date 2021-06-01
</code></pre>
<p>Let&#39;s proceed to the Airflow user interface (UI) - see next step.</p>
<p>Note that if you use <code>depends_on_past=True</code>, individual task instances will depend on the success of their previous task instance (that is, previous according to the logical date) In that case you may want to consider to set <code>wait_for_downstream=True</code> when using <code>depends_on_past=True</code>. While <code>depends_on_past=True</code> causes a task instance to depend on the success of its previous task_instance, <code>wait_for_downstream=True</code> will cause a task instance to also wait for all task instances immediately downstream of the previous task instance to succeed.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Airflow UI" duration="5">
        <ul>
<li>If the webserver is not already running, start it now:</li>
</ul>
<pre><code language="language-Bash" class="language-Bash">airflow webserver
</code></pre>
<p>Open the Airflow web interface in your browser:</p>
<ul>
<li><a href="http://0.0.0.0:8080/home" target="_blank">http://0.0.0.0:8080/home</a></li>
</ul>
<p>Now start experimenting with the Airflow web interface:</p>
<ul>
<li>Select <code>my_airflow_dag</code> from the list of DAGs.</li>
<li>Click on the icon &#34;Graph&#34; to display the DAG</li>
<li>Explore the other options to learn more about your DAG (see this Airflow tutorial about the <a href="https://airflow.apache.org/docs/apache-airflow/stable/ui.html" target="_blank">Airflow UI</a>)</li>
<li>If you are done:<ol type="1">
<li>Log out from the user menu,</li>
<li>Go to your terminal and stop the Airflow process with Ctrl+c (this will shut down all components).</li>
</ol>
</li>
</ul>


      </google-codelab-step>
    
      <google-codelab-step label="What&#39;s next?" duration="2">
        <p>Congratulations! You have completed the tutorial and learned how to:</p>
<p>✅ Install Apache Airflow<br> ✅ Start Apache Airflow<br> ✅ Create a simple pipeline</p>
<p>Next, you may want to proceed with this tutorial to build another DAG (there are more examples when you scroll down):</p>
<ul>
<li><a href="https://airflow.apache.org/docs/apache-airflow/stable/tutorial.html" target="_blank">Airflow DAG example</a></li>
</ul>
<p class="image-container"><img alt="Jan Kirenz" style="width: 100.00px" src="img/f80fc95e35c0d9fa.png"></p>
<p>Thank you for participating in this tutorial. If you found any issues along the way I&#39;d appreciate it if you&#39;d raise them by clicking the &#34;Report a mistake&#34; button at the bottom left of this site.</p>
<p><em>Jan Kirenz | </em><a href="https://www.kirenz.com" target="_blank"><em>kirenz.com</em></a><em> | </em><a href="https://creativecommons.org/licenses/by-nc/2.0/" target="_blank"><em>CC BY-NC 2.0 License</em></a></p>


      </google-codelab-step>
    
  </google-codelab>

  <script src="https://storage.googleapis.com/codelab-elements/native-shim.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/custom-elements.min.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/prettify.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/codelab-elements.js"></script>
  <script src="//support.google.com/inapp/api.js"></script>

</body>
</html>
