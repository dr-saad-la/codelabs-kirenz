
<!doctype html>

<html>
<head>
  <meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, user-scalable=yes">
  <meta name="theme-color" content="#4F7DC9">
  <meta charset="UTF-8">
  <title>Airflow installation tutorial</title>
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Code+Pro:400|Roboto:400,300,400italic,500,700|Roboto+Mono">
  <link rel="stylesheet" href="//fonts.googleapis.com/icon?family=Material+Icons">
  <link rel="stylesheet" href="https://storage.googleapis.com/codelab-elements/codelab-elements.css">
  <style>
    .success {
      color: #1e8e3e;
    }
    .error {
      color: red;
    }
  </style>
</head>
<body>
  <google-codelab-analytics gaid="UA-49880327-14"></google-codelab-analytics>
  <google-codelab codelab-gaid=""
                  id="airflow-setup"
                  title="Airflow installation tutorial"
                  environment="web"
                  feedback-link="https://github.com/kirenz/codelabs/blob/master/markdown/airflow-setup">
    
      <google-codelab-step label="Overview" duration="1">
        <h2 is-upgraded>What we cover</h2>
<p>In this tutorial we are going to install <a href="https://airflow.apache.org/docs/apache-airflow/stable/start/local.html" target="_blank">Apache Airflow</a> on your system.</p>
<p class="image-container"><img alt="Apache Airflow logo" style="width: 200.00px" src="img/86832b628c8aabfe.png"></p>
<ul>
<li>Airflow is a open source platform to programmatically author, schedule and monitor workflows.</li>
<li>Airflow pipelines are defined in Python, allowing for dynamic pipeline generation. This allows for writing code that instantiates pipelines dynamically.</li>
<li>Anyone with Python knowledge can deploy a workflow with Airflow. Apache Airflow does not limit the scope of your pipelines; you can use it to build ML models, transfer data, manage your infrastructure, and more.</li>
</ul>
<aside class="special"><p> Monitor, schedule and manage your workflows via a robust and modern web application.  </p>
</aside>


      </google-codelab-step>
    
      <google-codelab-step label="Prerequisites" duration="5">
        <h2 is-upgraded>Windows Subsystem for Linux</h2>
<p>If you have a Windows machine, you need Windows Subsystem for Linux. Follow the steps in this tutorial:</p>
<ul>
<li><a href="https://docs.microsoft.com/en-us/windows/wsl/about" target="_blank">What is the Windows Subsystem for Linux?</a></li>
</ul>
<p>Furthermore, take a look at this short answer at <a href="https://stackoverflow.com/a/53939373" target="_blank">stackoverflow</a> or the more detailed version in this <a href="https://coding-stream-of-consciousness.com/2018/11/06/apache-airflow-windows-10-install-ubuntu/" target="_blank">post</a>.</p>
<h2 is-upgraded>Anaconda</h2>
<p>To start this tutorial, I recommend to use Anaconda. If you don&#39;t already have Anaconda, go to <a href="https://www.anaconda.com/products/individual" target="_blank">anaconda.com</a> and choose the appropriate <code>Graphical Installer</code> for your system (Windows, MacOS or Linux). Install the software on your system:</p>
<ul>
<li><a href="https://docs.continuum.io/anaconda/install/mac-os/" target="_blank">Installing on macOS</a></li>
<li><a href="https://docs.continuum.io/anaconda/install/windows/" target="_blank">Installing on Windows</a></li>
<li><a href="https://docs.continuum.io/anaconda/install/linux/" target="_blank">https://docs.continuum.io/anaconda/install/linux/</a></li>
</ul>


      </google-codelab-step>
    
      <google-codelab-step label="Create Virtual Environment" duration="2">
        <p><a href="https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#creating-an-environment-with-commands" target="_blank">Conda environments</a> help manage dependencies and isolate projects. This is particularly useful when some packages require specific Python versions.</p>
<p>On Windows open the Start menu and open an Anaconda Command Prompt. On macOS or Linux open a terminal window.</p>
<p>We create an environment with a specific version of Python and install pip. We call the environment <code>airflow</code>:</p>
<pre><code language="language-bash" class="language-bash">conda create -n airflow python=3.9 pip
</code></pre>
<p>When conda asks you to proceed <code>(proceed ([y]/n)?</code>), type <code>y</code>.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Installation" duration="10">
        <p>To install Airflow, we mainly follow the <a href="https://airflow.apache.org/docs/apache-airflow/stable/start/local.html" target="_blank"> installation tutorial</a> provided by Apache Airflow. Note that we use <a href="https://pip.pypa.io/en/stable/" target="_blank"><strong>pip</strong></a> to install Airflow an some additional modules in our environment. When pip asks you to proceed <code>(proceed ([y]/n)?</code>), simply type <code>y</code>.</p>
<ul>
<li>First, you need to activate your environment as follows:</li>
</ul>
<pre><code language="language-bash" class="language-bash">conda activate airflow
</code></pre>
<ul>
<li>Then upgrade pip:</li>
</ul>
<pre><code language="language-bash" class="language-bash">pip install --upgrade pip
</code></pre>
<ul>
<li>Airflow needs <code>virualenv</code> so we install it:</li>
</ul>
<pre><code language="language-bash" class="language-bash">pip install virtualenv
</code></pre>
<ul>
<li>Next, Airflow needs a home. <code>your-home-directory/airflow</code> is the default:</li>
</ul>
<p><em>Here is the command for Mac and Linux:</em></p>
<pre><code language="language-bash" class="language-bash">export AIRFLOW_HOME=~/airflow
</code></pre>
<ul>
<li>Install Airflow with the following constraints file. We use Airflow Version &#34;2.3.0&#34; and Python &#34;3.9.&#34;:</li>
</ul>
<pre><code language="language-bash" class="language-bash">pip install &#34;apache-airflow==2.3.0&#34; --constraint &#34;https://raw.githubusercontent.com/apache/airflow/constraints-2.3.0/constraints-3.9.txt&#34;
</code></pre>
<ul>
<li>The following <code>airflow standalone</code> command will <ul>
<li>(1) initialise the database,</li>
<li>(2) make a user, and</li>
<li>(3) start all components for you</li>
</ul>
</li>
</ul>
<pre><code language="language-bash" class="language-bash">airflow standalone
</code></pre>
<p>We only run this command once when we install Airflow. If you want to run the individual parts of Airflow manually rather than using the all-in-one standalone command, check out the instructions provided <a href="https://airflow.apache.org/docs/apache-airflow/stable/start/local.html" target="_blank">here</a>.</p>
<aside class="warning"><p> If you get the error message &#34;AttributeError: &#39;NoneType object has no attribute is_alive&#34; stop the process with Ctrl + c and use the command airflow standalone one more time. </p>
</aside>
<ul>
<li>In the terminal output: Look for the provided <code>username</code> and <code>password</code> and store them somewhere</li>
<li>Open the Airflow UI in your browser (ideally in Chrome) <a href="http://0.0.0.0:8080" target="_blank">http://0.0.0.0:8080</a> and provide <code>username</code> and <code>password</code>.</li>
<li>The Airflow UI makes it easy to monitor and troubleshoot your data pipelines. Here&#39;s a quick overview of some of the features and visualizations you can find: <a href="https://airflow.apache.org/docs/apache-airflow/stable/ui.html#" target="_blank">Airflow UI</a></li>
<li>If you are done:<ol type="1">
<li>Log out from the user menu,</li>
<li>Go to your terminal and stop the Airflow process with <code>Ctrl</code>+<code>c</code> (this will shut down components).</li>
</ol>
<br>Later, we will restart Airflow by using different commands.</li>
</ul>


      </google-codelab-step>
    
      <google-codelab-step label="Start Airflow" duration="2">
        <p>In this section, we take a look at how to start Airflow:</p>
<ul>
<li>Start your terminal and activate your <code>airflow</code> environment if needed</li>
</ul>
<pre><code language="language-Bash" class="language-Bash">conda activate airflow
</code></pre>
<ul>
<li>Export the airflow home variable</li>
</ul>
<pre><code language="language-Bash" class="language-Bash">export AIRFLOW_HOME=~/airflow
</code></pre>
<ul>
<li>Start the Airflow webserver</li>
</ul>
<pre><code language="language-Bash" class="language-Bash">airflow webserver
</code></pre>
<ul>
<li>Open the Airflow UI in your browser (ideally in Chrome) <a href="http://0.0.0.0:8080" target="_blank">http://0.0.0.0:8080</a> and provide your <code>username</code> and <code>password</code>.</li>
<li>If you are done:<ol type="1">
<li>Log out from the user menu,</li>
<li>Go to your terminal and stop the Airflow process with <code>Ctrl</code>+<code>c</code> (this will shut down all components).</li>
</ol>
</li>
</ul>


      </google-codelab-step>
    
      <google-codelab-step label="First pipeline" duration="5">
        <p>Here, we mainly follow the instructions provided in this <a href="https://airflow.apache.org/docs/apache-airflow/stable/tutorial.html#tutorial" target="_blank">Apache Airflow tutorial</a>:</p>
<ul>
<li>First, create a new folder called <code>dags</code> in you airflow home (i.e. <code>~/airflow/dags</code>).</li>
<li>Copy <a href="https://github.com/kirenz/airflow/blob/main/tutorial.py" target="_blank">this Python script</a> and save it as <code>my_airflow_dag.py</code> in your <code>~/airflow/dags</code> folder.</li>
<li>Copy <a href="https://github.com/kirenz/airflow/blob/main/templated_command.sh" target="_blank">this shell script</a> and save it as <code>templated_command.sh</code> in your <code>~/airflow/dags</code> folder.</li>
</ul>
<aside class="warning"><p> The file my_airflow_dag needs to be stored in the DAGs folder referenced in your airflow.cfg. The default location for your DAGs is ~/airflow/dags. </p>
</aside>
<ul>
<li>Open a new terminal window and activate your <code>airflow</code> environment if needed</li>
</ul>
<pre><code language="language-Bash" class="language-Bash">conda activate airflow
</code></pre>
<ul>
<li>Export the airflow home variable</li>
</ul>
<pre><code language="language-Bash" class="language-Bash">export AIRFLOW_HOME=~/airflow
</code></pre>
<ul>
<li>Now run the following command:</li>
</ul>
<pre><code language="language-bash" class="language-bash">python ~/airflow/dags/my_airflow_dag.py
</code></pre>
<p>If the script does not raise an exception it means that you have not done anything wrong, and that your Airflow environment is somewhat sound.</p>
<ul>
<li>Now proceed to the next step.</li>
</ul>
<p><em>If you want to learn more about the content of the my_airflow-dag.py script, review </em><a href="https://airflow.apache.org/docs/apache-airflow/stable/tutorial.html" target="_blank"><em>the Airflow tutorial</em></a><em>.</em></p>


      </google-codelab-step>
    
      <google-codelab-step label="Command Line Metadata Validation" duration="4">
        <p>First, we use the command line to do some metadata validation. Let&#39;s run a few commands in your terminal to test your script:</p>
<ul>
<li>Initialize the database tables</li>
</ul>
<pre><code language="language-bash" class="language-bash">airflow db init
</code></pre>
<ul>
<li>Print the list of active DAGs (there are many example DAGs provided by Airflow)</li>
</ul>
<pre><code language="language-bash" class="language-bash">airflow dags list
</code></pre>
<ul>
<li>Print the list of tasks in the &#34;my_airflow_dag&#34;</li>
</ul>
<pre><code language="language-bash" class="language-bash">airflow tasks list my_airflow_dag
</code></pre>
<ul>
<li>Print the hierarchy of tasks in the &#34;my_airflow_dag&#34; DAG</li>
</ul>
<pre><code language="language-bash" class="language-bash">airflow tasks list my_airflow_dag --tree
</code></pre>


      </google-codelab-step>
    
      <google-codelab-step label="Testing single tasks" duration="4">
        <p>Let&#39;s start our tests by running one actual task instance for a specific date (independent of other tasks).</p>
<p>The date specified in this context is called the &#34;logical date&#34; (also called execution date), which simulates the scheduler running your task or DAG for a specific date and time, even though it physically will run now (or as soon as its dependencies are met).</p>
<aside class="special"><p> The scheduler runs your task for a specific date and time, not at a specific date.  </p>
</aside>
<p>This is because each run of a DAG conceptually represents not a specific date and time, but an interval between two times, called a <em>data interval</em>. A DAG run&#39;s logical date is the start of its data interval.</p>
<p>The general command layout is as follows:</p>
<pre><code language="language-Bash" class="language-Bash">command subcommand dag_id task_id date
</code></pre>
<ul>
<li>Testing <code>task_print_date</code>:</li>
</ul>
<pre><code language="language-bash" class="language-bash">airflow tasks test my_airflow_dag task_print_date 2022-05-20
</code></pre>
<p><em>Take a look at the last lines in the output (ignore warnings for now)</em></p>
<ul>
<li>Testing <code>task_sleep</code></li>
</ul>
<pre><code language="language-bash" class="language-bash">airflow tasks test my_airflow_dag task_sleep 2022-05-20
</code></pre>
<ul>
<li>Testing <code>task_templated</code></li>
</ul>
<pre><code language="language-Bash" class="language-Bash">airflow tasks test my_airflow_dag task_templated 2022-05-20
</code></pre>
<p>Everything looks like it&#39;s running fine so let&#39;s run a backfill.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Backfill" duration="4">
        <p><code>backfill</code> will respect your dependencies, emit logs into files and talk to the database to record status.</p>
<p>If you do have a <em>webserver</em> up, you will be able to track the progress.</p>
<p><code>airflow webserver</code> will start a web server if you are interested in tracking the progress visually as your backfill progresses.</p>
<ul>
<li>The date range in this context is a <code>start_date</code> and optionally an <code>end_date</code>, which are used to populate the run schedule with task instances from this dag.</li>
<li>Optional, start a web server in debug mode in the background</li>
</ul>
<pre><code language="language-Bash" class="language-Bash">airflow webserver --debug &amp;
</code></pre>
<ul>
<li>Start your backfill on a date range</li>
</ul>
<pre><code language="language-Bash" class="language-Bash">airflow dags backfill my_airflow_dag \
    --start-date 2022-05-20 \
    --end-date 2022-06-22
</code></pre>
<p>Let&#39;s proceed to the Airflow user interface (UI).</p>
<aside class="special"><p> If you use depends_on_past=True, individual task instances will depend on the success of their previous task instance (that is, previous according to the logical date). </p>
</aside>
<p>Note that you may want to consider <code>wait_for_downstream=True</code> when using <code>depends_on_past=True</code>. While <code>depends_on_past=True</code> causes a task instance to depend on the success of its previous task_instance, <code>wait_for_downstream=True</code> will cause a task instance to also wait for all task instances immediately downstream of the previous task instance to succeed.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Airflow UI" duration="5">
        <p>Open the Airflow web interface in your browser:</p>
<ul>
<li><a href="http://0.0.0.0:8080/home" target="_blank">http://0.0.0.0:8080/home</a></li>
</ul>
<p>Now start experimenting with the Airflow web interface:</p>
<ul>
<li>Select <code>my_airflow_dag</code> from the list of DAGs.</li>
<li>Click on the icon &#34;Graph&#34; to display the DAG</li>
<li>Explore the other options to learn more about your DAG (see this Airflow tutorial about the <a href="https://airflow.apache.org/docs/apache-airflow/stable/ui.html" target="_blank">Airflow UI</a>)</li>
<li>If you are done:<ol type="1">
<li>Log out from the user menu,</li>
<li>Go to your terminal and stop the Airflow process with Ctrl+c (this will shut down all components).</li>
</ol>
</li>
</ul>


      </google-codelab-step>
    
      <google-codelab-step label="What&#39;s next?" duration="2">
        <p>Congratulations! You have completed the tutorial and learned how to:</p>
<p>✅ Install Apache Airflow<br> ✅ Start Apache Airflow<br> ✅ Create a simple pipeline</p>
<p>Next, you may want to proceed with this tutorial to build another DAG:</p>
<ul>
<li><a href="https://airflow.apache.org/docs/apache-airflow/stable/tutorial.html" target="_blank">Airflow DAG example</a></li>
</ul>
<p class="image-container"><img alt="Jan Kirenz" style="width: 100.00px" src="img/f80fc95e35c0d9fa.png"></p>
<p>Thank you for participating in this tutorial. If you found any issues along the way I&#39;d appreciate it if you&#39;d raise them by clicking the &#34;Report a mistake&#34; button at the bottom left of this site.</p>
<p><em>Copyright: Jan Kirenz (2021) | </em><a href="https://www.kirenz.com" target="_blank"><em>kirenz.com</em></a><em> | </em><a href="https://creativecommons.org/licenses/by-nc/2.0/" target="_blank"><em>CC BY-NC 2.0 License</em></a></p>


      </google-codelab-step>
    
  </google-codelab>

  <script src="https://storage.googleapis.com/codelab-elements/native-shim.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/custom-elements.min.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/prettify.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/codelab-elements.js"></script>
  <script src="//support.google.com/inapp/api.js"></script>

</body>
</html>
